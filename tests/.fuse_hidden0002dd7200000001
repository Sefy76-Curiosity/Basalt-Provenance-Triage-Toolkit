#!/usr/bin/env python3
"""
SCIENTIFIC TOOLKIT - COMPREHENSIVE TEST SUITE
============================================
Tests the actual application functionality and generates detailed reports.

Usage:
    python test_toolkit.py                    # Run all tests
    python test_toolkit.py --category core    # Run specific category
    python test_toolkit.py --report           # Generate only report from last run
    python test_toolkit.py --verbose          # Show detailed output

Reports are saved to: test_reports/[category]_[timestamp].txt
"""

import os
import sys
import json
import time
import argparse
import traceback
import importlib.util
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from collections import Counter

# Add the toolkit directory to path so we can import actual modules
SCRIPT_DIR = Path(__file__).parent.absolute()
sys.path.insert(0, str(SCRIPT_DIR))

# Try to import actual toolkit modules
try:
    from data_hub import DataHub
    from engines.classification_engine import ClassificationEngine
    TOOLKIT_AVAILABLE = True
    print("‚úÖ Successfully imported toolkit modules")
except ImportError as e:
    TOOLKIT_AVAILABLE = False
    print(f"‚ö†Ô∏è Could not import toolkit modules: {e}")
    print("   Some tests will be skipped")

# ============================================================================
# TEST REPORTING
# ============================================================================

class TestReport:
    def __init__(self, category: str, verbose=False):
        self.category = category
        self.verbose = verbose
    """Generates detailed test reports"""

    def __init__(self, category: str):
        self.category = category
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results = []
        self.start_time = time.time()
        self.report_dir = Path("test_reports")
        self.report_dir.mkdir(exist_ok=True)

    def add_result(self, test_name: str, passed: bool, details: str = "", error: str = ""):
        """Add a test result"""
        self.results.append({
            'name': test_name,
            'passed': passed,
            'details': details,
            'error': error,
            'time': time.time() - self.start_time
        })

    def generate(self) -> Path:
        """Generate the report file"""
        elapsed = time.time() - self.start_time

        # Calculate statistics
        total = len(self.results)
        passed = sum(1 for r in self.results if r['passed'])
        failed = total - passed

        # Generate filename
        filename = self.report_dir / f"{self.category}_{self.timestamp}.txt"

        with open(filename, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"SCIENTIFIC TOOLKIT TEST REPORT\n")
            f.write(f"Category: {self.category.upper()}\n")
            f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Duration: {elapsed:.2f} seconds\n")
            f.write("=" * 80 + "\n\n")

            f.write(f"SUMMARY\n")
            f.write("-" * 40 + "\n")
            f.write(f"Total Tests: {total}\n")
            f.write(f"Passed:      {passed}\n")
            f.write(f"Failed:      {failed}\n")
            f.write(f"Success Rate: {(passed/total*100):.1f}%\n\n")

            if failed > 0:
                f.write("FAILED TESTS\n")
                f.write("-" * 40 + "\n")
                for r in self.results:
                    if not r['passed']:
                        f.write(f"\n‚ùå {r['name']}\n")
                        if r['error']:
                            f.write(f"   Error: {r['error']}\n")
                        if r['details']:
                            f.write(f"   Details: {r['details']}\n")
                f.write("\n")

            f.write("DETAILED RESULTS\n")
            f.write("-" * 40 + "\n")
            for i, r in enumerate(self.results, 1):
                status = "‚úÖ PASS" if r['passed'] else "‚ùå FAIL"
                f.write(f"\n{status} [{i:2d}] {r['name']}\n")
                if r['details']:
                    f.write(f"      {r['details']}\n")
                if r['error'] and not r['passed']:
                    f.write(f"      Error: {r['error']}\n")

            f.write("\n" + "=" * 80 + "\n")
            f.write("END OF REPORT\n")
            f.write("=" * 80 + "\n")

        print(f"\nüìÑ Report saved to: {filename}")
        return filename

# ============================================================================
# TEST RUNNER
# ============================================================================

class TestRunner:
    """Runs tests and manages reporting"""

    def __init__(self, verbose=False):
        self.verbose = verbose
        self.reports = {}
        self.start_time = time.time()

    def print_header(self, text):
        """Print a section header"""
        print(f"\n{'='*70}")
        print(f" {text}")
        print(f"{'='*70}")

    def print_subheader(self, text):
        """Print a subsection header"""
        print(f"\n‚ñ∂ {text}")

    def run_category(self, name: str, test_func) -> TestReport:
        """Run a test category and generate report"""
        print(f"\nüìã Running {name.upper()} tests...")
        report = TestReport(name)

        try:
            test_func(report)
        except Exception as e:
            report.add_result(
                f"{name}_test_suite",
                False,
                error=f"Test suite crashed: {str(e)}\n{traceback.format_exc()}"
            )
            print(f"‚ùå Test suite crashed: {e}")

        # Generate and store report
        report_path = report.generate()
        self.reports[name] = report_path

        # Print quick summary
        passed = sum(1 for r in report.results if r['passed'])
        total = len(report.results)
        print(f"  {passed}/{total} tests passed ({passed/total*100:.1f}%)")

        return report

    def print_summary(self):
        """Print overall test summary"""
        elapsed = time.time() - self.start_time

        print(f"\n{'='*70}")
        print(f" TEST SUITE COMPLETE")
        print(f"{'='*70}")
        print(f"‚è±Ô∏è  Total time: {elapsed:.2f} seconds")
        print(f"\nüìä Category Reports:")

        total_passed = 0
        total_tests = 0

        for category, report_path in self.reports.items():
            # Count tests in report (quick estimate by reading file)
            try:
                with open(report_path, 'r') as f:
                    content = f.read()
                    passed = content.count("‚úÖ PASS")
                    failed = content.count("‚ùå FAIL")
                    total_passed += passed
                    total_tests += passed + failed
                    print(f"  {category:12} : {passed}/{passed+failed} passed - {report_path.name}")
            except:
                print(f"  {category:12} : Report: {report_path.name}")

        if total_tests > 0:
            print(f"\n{'‚îÄ'*50}")
            print(f"  OVERALL: {total_passed}/{total_tests} passed ({total_passed/total_tests*100:.1f}%)")

        if total_passed == total_tests:
            print(f"\n‚úÖ‚úÖ‚úÖ ALL TESTS PASSED! ‚úÖ‚úÖ‚úÖ")
        else:
            print(f"\n‚ö†Ô∏è  Some tests failed - check reports for details")

# ============================================================================
# ACTUAL TEST FUNCTIONS
# ============================================================================

def test_data_hub(report: TestReport):
    """Test DataHub functionality"""

    if not TOOLKIT_AVAILABLE:
        report.add_result("DataHub Import", False, error="Toolkit modules not available")
        return

    # Create DataHub instance
    hub = DataHub()

    # Test 1: Add samples
    samples = [
        {'Sample_ID': 'TEST001', 'Zr_ppm': 150, 'Nb_ppm': 15, 'Notes': 'Test sample 1'},
        {'Sample_ID': 'TEST002', 'Zr_ppm': 200, 'Nb_ppm': 20, 'Notes': 'Test sample 2'},
    ]

    try:
        count = hub.add_samples(samples)
        report.add_result(
            "Add samples",
            count == 2,
            details=f"Added {count} samples"
        )
    except Exception as e:
        report.add_result("Add samples", False, error=str(e))

    # Test 2: Verify row count
    try:
        row_count = hub.row_count()
        report.add_result(
            "Row count",
            row_count == 2,
            details=f"Got {row_count} rows"
        )
    except Exception as e:
        report.add_result("Row count", False, error=str(e))

    # Test 3: Get all samples
    try:
        all_samples = hub.get_all()
        report.add_result(
            "Get all samples",
            len(all_samples) == 2 and all_samples[0]['Sample_ID'] == 'TEST001',
            details=f"Retrieved {len(all_samples)} samples"
        )
    except Exception as e:
        report.add_result("Get all samples", False, error=str(e))

    # Test 4: Get by ID
    try:
        sample = hub.get_by_id('TEST001')
        report.add_result(
            "Get by ID",
            sample is not None and sample['Sample_ID'] == 'TEST001',
            details=f"Found sample: {sample.get('Sample_ID') if sample else 'None'}"
        )
    except Exception as e:
        report.add_result("Get by ID", False, error=str(e))

    # Test 5: Update row
    try:
        hub.update_row(0, {'Zr_ppm': 175, 'Notes': 'Updated'})
        updated = hub.get_all()[0]
        report.add_result(
            "Update row",
            updated['Zr_ppm'] == 175 and updated['Notes'] == 'Updated',
            details=f"Zr_ppm: {updated['Zr_ppm']}, Notes: {updated['Notes']}"
        )
    except Exception as e:
        report.add_result("Update row", False, error=str(e))

    # Test 6: Get page
    try:
        page = hub.get_page(0, page_size=1)
        report.add_result(
            "Get page",
            len(page) == 1 and page[0]['Sample_ID'] == 'TEST001',
            details=f"Page has {len(page)} samples"
        )
    except Exception as e:
        report.add_result("Get page", False, error=str(e))

    # Test 7: Column names
    try:
        columns = hub.get_column_names()
        expected = {'Sample_ID', 'Zr_ppm', 'Nb_ppm', 'Notes'}
        report.add_result(
            "Column names",
            set(columns).issuperset(expected),
            details=f"Columns: {columns}"
        )
    except Exception as e:
        report.add_result("Column names", False, error=str(e))

    # Test 8: Delete rows
    try:
        hub.delete_rows([0])
        remaining = hub.row_count()
        report.add_result(
            "Delete rows",
            remaining == 1,
            details=f"{remaining} rows remaining after deletion"
        )
    except Exception as e:
        report.add_result("Delete rows", False, error=str(e))

    # Test 9: Unsaved changes tracking
    try:
        hub.mark_unsaved()
        has_unsaved = hub.has_unsaved_changes()
        hub.mark_saved()
        has_saved = hub.has_unsaved_changes()
        report.add_result(
            "Unsaved changes tracking",
            has_unsaved and not has_saved,
            details=f"Unsaved: {has_unsaved}, After save: {has_saved}"
        )
    except Exception as e:
        report.add_result("Unsaved changes tracking", False, error=str(e))

    # Test 10: Clear all
    try:
        hub.clear_all()
        empty_count = hub.row_count()
        report.add_result(
            "Clear all",
            empty_count == 0,
            details=f"Rows after clear: {empty_count}"
        )
    except Exception as e:
        report.add_result("Clear all", False, error=str(e))


def test_classification_engine(report: TestReport):
    """Test ClassificationEngine functionality"""

    if not TOOLKIT_AVAILABLE:
        report.add_result("Classification Engine Import", False, error="Toolkit modules not available")
        return

    try:
        # Initialize engine
        engine = ClassificationEngine()
        report.add_result(
            "Engine initialization",
            engine is not None,
            details="Successfully created ClassificationEngine"
        )
    except Exception as e:
        report.add_result("Engine initialization", False, error=str(e))
        return  # Can't continue if engine won't initialize

    # Test 1: Get available schemes
    try:
        schemes = engine.get_available_schemes()
        scheme_count = len(schemes)
        report.add_result(
            "Get available schemes",
            scheme_count > 0,
            details=f"Found {scheme_count} classification schemes"
        )

        if scheme_count > 0:
            # Just report the count, don't try to print schemes
            pass

    except Exception as e:
        report.add_result("Get available schemes", False, error=str(e))

    # Test 2: Get scheme info
    if schemes:
        try:
            first_scheme = schemes[0]['id']
            info = engine.get_scheme_info(first_scheme)
            report.add_result(
                "Get scheme info",
                info and info.get('name') == schemes[0]['name'],
                details=f"Retrieved info for: {info.get('name')}"
            )
        except Exception as e:
            report.add_result("Get scheme info", False, error=str(e))

    # Test 3: Classify a sample
    test_sample = {
        'Sample_ID': 'TEST001',
        'Zr_ppm': 165,
        'Nb_ppm': 18,
        'Ti_ppm': 9500,
        'Ba_ppm': 260,
        'Rb_ppm': 15,
        'Cr_ppm': 1800,
        'Ni_ppm': 1400
    }

    if schemes:
        try:
            scheme_id = schemes[0]['id']
            name, confidence, color, derived = engine.classify_sample(test_sample, scheme_id)
            report.add_result(
                f"Classify sample with {schemes[0]['name']}",
                name not in ['UNCLASSIFIED', 'SCHEME_NOT_FOUND', 'INVALID_SAMPLE'],
                details=f"Result: {name} (conf: {confidence})"
            )
        except Exception as e:
            report.add_result("Classify sample", False, error=str(e))

    # Test 4: Classify all samples
    samples = [
        {'Sample_ID': 'TEST001', 'Zr_ppm': 165, 'Nb_ppm': 18},
        {'Sample_ID': 'TEST002', 'Zr_ppm': 200, 'Nb_ppm': 25},
        {'Sample_ID': 'TEST003', 'Zr_ppm': 50, 'Nb_ppm': 2},
    ]

    if schemes:
        try:
            scheme_id = schemes[0]['id']
            results = engine.classify_all_samples(samples, scheme_id)
            report.add_result(
                "Classify all samples",
                len(results) == len(samples),
                details=f"Classified {len(results)} samples"
            )

            # Check if any were classified
            classified = sum(1 for r in results if r['classification'] not in ['UNCLASSIFIED', 'INVALID_SAMPLE'])
            if classified > 0:
                report.add_result(
                    "Classification success rate",
                    True,
                    details=f"{classified}/{len(samples)} samples classified"
                )
            else:
                report.add_result(
                    "Classification success rate",
                    False,
                    details=f"WARNING: No samples classified - check if required fields are present"
                )
        except Exception as e:
            report.add_result("Classify all samples", False, error=str(e))

    # Test 5: Test with real bone collagen sample
    if 'bone_collagen_qc' in [s['id'] for s in schemes]:
        try:
            bone_sample = {'C_N_Ratio': 3.2, 'Sample_ID': 'BONE001'}
            name, confidence, color, derived = engine.classify_sample(bone_sample, 'bone_collagen_qc')
            report.add_result(
                "Bone Collagen QC - Good sample",
                "PRESERVED" in name,
                details=f"Result: {name}"
            )
        except Exception as e:
            report.add_result("Bone Collagen QC", False, error=str(e))

    # Test 6: Test with degraded bone
    if 'bone_collagen_qc' in [s['id'] for s in schemes]:
        try:
            degraded_sample = {'C_N_Ratio': 5.8, 'Sample_ID': 'BONE002'}
            name, confidence, color, derived = engine.classify_sample(degraded_sample, 'bone_collagen_qc')
            report.add_result(
                "Bone Collagen QC - Degraded sample",
                "DEGRADED" in name,
                details=f"Result: {name}"
            )
        except Exception as e:
            report.add_result("Bone Collagen QC - Degraded", False, error=str(e))

    # Test 7: Test TAS classification
    if 'tas_alkali_series' in [s['id'] for s in schemes]:
        try:
            # Test alkaline sample
            alkaline = {'Na2O_wt': 9.0, 'K2O_wt': 5.0, 'SiO2_wt': 57.0}
            name, conf, color, derived = engine.classify_sample(alkaline, 'tas_alkali_series')
            report.add_result(
                "TAS - Alkaline series",
                "ALKALINE" in name,
                details=f"Total alkali: 14.0 ‚Üí {name}"
            )

            # Test subalkaline sample
            subalkaline = {'Na2O_wt': 2.5, 'K2O_wt': 0.15, 'SiO2_wt': 50.0}
            name, conf, color, derived = engine.classify_sample(subalkaline, 'tas_alkali_series')
            report.add_result(
                "TAS - Subalkaline series",
                "SUBALKALINE" in name,
                details=f"Total alkali: 2.65 ‚Üí {name}"
            )
        except Exception as e:
            report.add_result("TAS tests", False, error=str(e))

    # Test 8: Test derived fields
    try:
        sample_with_ratios = {
            'Zr_ppm': 200,
            'Nb_ppm': 10,
            'Al2O3_wt': 15,
            'CaO_wt': 5,
            'Na2O_wt': 3,
            'K2O_wt': 5,
            'Yb_ppm': 3,
            'Th_ppm': 2
        }

        # Compute derived fields
        derived = engine._compute_derived_fields(sample_with_ratios)

        tests = [
            ('Zr_Nb_Ratio', derived.get('Zr_Nb_Ratio'), 20.0),
            ('Total_Alkali', derived.get('Total_Alkali'), 8.0),
            ('Nb_Yb_Ratio', derived.get('Nb_Yb_Ratio'), 200/3 if 200 in sample_with_ratios.values() else None),
        ]

        for field_name, value, expected in tests:
            if value is not None:
                report.add_result(
                    f"Derived field: {field_name}",
                    abs(value - expected) < 0.001 if expected else True,
                    details=f"{field_name} = {value:.2f}"
                )
    except Exception as e:
        report.add_result("Derived fields", False, error=str(e))


def test_scheme_files(report: TestReport):
    """Test actual JSON scheme files"""

    # Look for scheme files in various locations
    scheme_paths = [
        Path("engines/classification"),
        Path("../engines/classification"),
        Path("engines/more_classifications"),
    ]

    found_schemes = []

    for base_path in scheme_paths:
        if base_path.exists() and base_path.is_dir():
            json_files = list(base_path.glob("*.json"))
            found_schemes.extend(json_files)

    if not found_schemes:
        report.add_result(
            "Find scheme files",
            False,
            details="No JSON scheme files found in expected locations",
            error="Check if engines/classification/ exists"
        )
        return

    report.add_result(
        "Find scheme files",
        len(found_schemes) > 0,
        details=f"Found {len(found_schemes)} JSON files"
    )

    # Test each scheme file
    valid_count = 0
    for scheme_file in found_schemes[:10]:  # Test first 10 to avoid overwhelming
        try:
            with open(scheme_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Check required fields
            required = ['scheme_name', 'version', 'classifications', 'requires_fields']
            missing = [f for f in required if f not in data]

            if not missing:
                valid_count += 1
                report.add_result(
                    f"Validate: {scheme_file.name}",
                    True,
                    details=f"Valid scheme with {len(data.get('classifications', []))} classifications"
                )
            else:
                report.add_result(
                    f"Validate: {scheme_file.name}",
                    False,
                    details=f"Missing fields: {missing}"
                )
        except json.JSONDecodeError as e:
            report.add_result(
                f"Validate: {scheme_file.name}",
                False,
                error=f"Invalid JSON: {e}"
            )
        except Exception as e:
            report.add_result(
                f"Validate: {scheme_file.name}",
                False,
                error=str(e)
            )

    if len(found_schemes) > 10:
        report.add_result(
            f"Additional schemes",
            True,
            details=f"{len(found_schemes)-10} more schemes not individually tested"
        )


def test_column_normalization(report: TestReport):
    """Test column name normalization"""

    # Load column mappings if available
    config_file = Path("config/chemical_elements.json")
    mappings = {}

    if config_file.exists():
        try:
            with open(config_file, 'r') as f:
                data = json.load(f)
                elements = data.get("elements", {})
                for elem, info in elements.items():
                    standard = info["standard"]
                    for var in info["variations"]:
                        mappings[var.lower()] = standard
            report.add_result(
                "Load column mappings",
                len(mappings) > 0,
                details=f"Loaded {len(mappings)} mappings"
            )
        except Exception as e:
            report.add_result("Load column mappings", False, error=str(e))
    else:
        report.add_result(
            "Load column mappings",
            False,
            details="chemical_elements.json not found",
            error="Run from toolkit root directory"
        )
        return

    # Define normalization function (copied from left_panel.py)
    def normalize_column_name(name, mappings):
        import re
        if not name:
            return name

        cleaned = str(name).strip()
        lookup_key = cleaned.lower()
        lookup_key = re.sub(r'\s+', ' ', lookup_key)

        if lookup_key in mappings:
            return mappings[lookup_key]

        lookup_key_underscore = lookup_key.replace(' ', '_')
        if lookup_key_underscore in mappings:
            return mappings[lookup_key_underscore]

        lookup_key_nosep = re.sub(r'[\s_]+', '', lookup_key)
        if lookup_key_nosep in mappings:
            return mappings[lookup_key_nosep]

        return re.sub(r'\s+', '_', cleaned)

    # Test cases
    test_cases = [
        # (input, expected)
        ("Zr ppm", "Zr_ppm"),
        ("Zr (ppm)", "Zr_ppm"),
        ("zr_ppm", "Zr_ppm"),
        ("ZR PPM", "Zr_ppm"),
        ("SiO2 (%)", "SiO2_wt"),
        ("SiO2 %", "SiO2_wt"),
        ("SiO2_wt%", "SiO2_wt"),
        ("Na2O (%)", "Na2O_wt"),
        ("K2O%", "K2O_wt"),
    ]

    passed = 0
    for input_name, expected in test_cases:
        result = normalize_column_name(input_name, mappings)
        if result == expected:
            passed += 1
            report.add_result(
                f"Normalize: '{input_name}'",
                True,
                details=f"‚Üí '{result}'"
            )
        else:
            report.add_result(
                f"Normalize: '{input_name}'",
                False,
                details=f"Expected '{expected}', got '{result}'"
            )

    # Test unknown column
    unknown = normalize_column_name("Some_Random_Column", mappings)
    report.add_result(
        "Unknown column",
        unknown == "Some_Random_Column",
        details=f"‚Üí '{unknown}'"
    )


def test_file_import(report: TestReport):
    """Test file import functionality with sample data"""

    # Create a temporary CSV file for testing
    import tempfile
    import csv

    temp_dir = tempfile.mkdtemp()
    test_csv = Path(temp_dir) / "test_data.csv"

    try:
        # Create test CSV
        with open(test_csv, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Sample_ID', 'Zr (ppm)', 'Nb (ppm)', 'Notes'])
            writer.writerow(['TEST001', '165', '18', 'First test'])
            writer.writerow(['TEST002', '200', '25', 'Second test'])
            writer.writerow(['TEST003', '50', '2', 'Third test'])

        report.add_result(
            "Create test CSV",
            test_csv.exists(),
            details=f"Created: {test_csv}"
        )

        # Try to parse with CSV module
        try:
            import csv
            with open(test_csv, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                rows = list(reader)

            report.add_result(
                "Parse CSV with csv module",
                len(rows) == 3,
                details=f"Parsed {len(rows)} rows"
            )
        except Exception as e:
            report.add_result("Parse CSV with csv module", False, error=str(e))

        # Try with pandas if available
        try:
            import pandas as pd
            df = pd.read_csv(test_csv)
            report.add_result(
                "Parse CSV with pandas",
                len(df) == 3,
                details=f"DataFrame shape: {df.shape}"
            )
        except ImportError:
            report.add_result(
                "Parse CSV with pandas",
                False,
                details="pandas not installed",
                error="Install with: pip install pandas"
            )
        except Exception as e:
            report.add_result("Parse CSV with pandas", False, error=str(e))

    finally:
        # Clean up
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        report.add_result(
            "Cleanup temp files",
            True,
            details="Removed temporary files"
        )


def test_ui_components(report: TestReport):
    """Test UI component structure without creating actual windows"""

    # Check if required UI files exist
    ui_files = [
        Path("ui/left_panel.py"),
        Path("ui/center_panel.py"),
        Path("ui/right_panel.py"),
        Path("ui/all_schemes_detail_dialog.py"),
    ]

    for ui_file in ui_files:
        exists = ui_file.exists()
        report.add_result(
            f"UI file: {ui_file.name}",
            exists,
            details=f"Path: {ui_file}" if exists else "File not found"
        )

    # Check if we can import them
    for ui_file in ui_files:
        if ui_file.exists():
            try:
                spec = importlib.util.spec_from_file_location(
                    ui_file.stem, ui_file
                )
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    report.add_result(
                        f"Import: {ui_file.stem}",
                        True,
                        details="Successfully imported"
                    )
                else:
                    report.add_result(
                        f"Import: {ui_file.stem}",
                        False,
                        error="Could not create module spec"
                    )
            except Exception as e:
                report.add_result(
                    f"Import: {ui_file.stem}",
                    False,
                    error=str(e)
                )


def test_plugin_structure(report: TestReport):
    """Test plugin directory structure"""

    plugin_dirs = [
        Path("plugins/software"),
        Path("plugins/hardware"),
        Path("plugins/add-ons"),
        Path("config/enabled_plugins.json"),
    ]

    for plugin_dir in plugin_dirs:
        exists = plugin_dir.exists()
        report.add_result(
            f"Plugin path: {plugin_dir}",
            exists,
            details="Exists" if exists else "Not found"
        )

    # Check enabled_plugins.json format if it exists
    enabled_plugins = Path("config/enabled_plugins.json")
    if enabled_plugins.exists():
        try:
            with open(enabled_plugins, 'r') as f:
                data = json.load(f)
            report.add_result(
                "enabled_plugins.json format",
                isinstance(data, dict),
                details=f"Contains {len(data)} plugins"
            )
        except json.JSONDecodeError as e:
            report.add_result(
                "enabled_plugins.json format",
                False,
                error=f"Invalid JSON: {e}"
            )
        except Exception as e:
            report.add_result(
                "enabled_plugins.json format",
                False,
                error=str(e)
            )


def test_derived_fields_calculations(report: TestReport):
    """Test specific derived field calculations"""

    # Load derived_fields.json if it exists
    derived_file = Path("engines/derived_fields.json")

    if not derived_file.exists():
        report.add_result(
            "Find derived_fields.json",
            False,
            details="File not found",
            error="Run from toolkit root directory"
        )
        return

    try:
        with open(derived_file, 'r') as f:
            data = json.load(f)

        fields = data.get('fields', [])
        report.add_result(
            "Load derived_fields.json",
            len(fields) > 0,
            details=f"Loaded {len(fields)} field definitions"
        )
    except Exception as e:
        report.add_result("Load derived_fields.json", False, error=str(e))
        return

    # Test each field definition
    for field in fields[:5]:  # Test first 5
        name = field.get('name')
        requires = field.get('requires', [])
        formula = field.get('formula', '')

        report.add_result(
            f"Field definition: {name}",
            bool(name and requires and formula),
            details=f"Requires: {requires}"
        )

    # Test actual calculations with sample data
    test_sample = {
        'Zr_ppm': 200,
        'Nb_ppm': 10,
        'Al2O3_wt': 15,
        'CaO_wt': 5,
        'Na2O_wt': 3,
        'K2O_wt': 5,
        'Yb_ppm': 3,
        'Th_ppm': 2,
        'Zr_error': '¬±5.2',
    }

    # Create a mini classification engine to test calculations
    try:
        from engines.classification_engine import ClassificationEngine
        engine = ClassificationEngine()

        # Test derived field computation
        derived = engine._compute_derived_fields(test_sample)

        # Test specific calculations
        if 'Zr_Nb_Ratio' in derived:
            expected = test_sample['Zr_ppm'] / test_sample['Nb_ppm']
            report.add_result(
                "Zr/Nb ratio calculation",
                abs(derived['Zr_Nb_Ratio'] - expected) < 0.001,
                details=f"Zr/Nb = {derived['Zr_Nb_Ratio']:.2f} (expected {expected:.2f})"
            )

        if 'Total_Alkali' in derived:
            expected = test_sample['Na2O_wt'] + test_sample['K2O_wt']
            report.add_result(
                "Total Alkali calculation",
                abs(derived['Total_Alkali'] - expected) < 0.001,
                details=f"Total Alkali = {derived['Total_Alkali']:.2f}"
            )

        # Test error field cleaning
        cleaned = engine._clean_error_fields(test_sample)
        if 'Zr_error' in cleaned:
            report.add_result(
                "Error field cleaning",
                cleaned['Zr_error'] == 5.2,
                details=f"Cleaned Zr_error: {cleaned['Zr_error']}"
            )

    except Exception as e:
        report.add_result("Derived field calculations", False, error=str(e))


def test_scientific_classifications(report: TestReport):
    """Test specific scientific classification schemes"""

    if not TOOLKIT_AVAILABLE:
        report.add_result("Scientific classifications", False, error="Toolkit modules not available")
        return

    try:
        engine = ClassificationEngine()
        schemes = engine.get_available_schemes()
        scheme_dict = {s['id']: s for s in schemes}
    except Exception as e:
        report.add_result("Initialize engine", False, error=str(e))
        return

    # Test 1: Bone Collagen QC
    if 'bone_collagen_qc' in scheme_dict:
        tests = [
            ({"C_N_Ratio": 3.2}, "PRESERVED", "Good bone"),
            ({"C_N_Ratio": 2.9}, "PRESERVED", "Lower boundary"),
            ({"C_N_Ratio": 3.6}, "PRESERVED", "Upper boundary"),
            ({"C_N_Ratio": 4.5}, "DEGRADED", "Degraded - high"),
            ({"C_N_Ratio": 2.0}, "DEGRADED", "Degraded - low"),
        ]

        for sample, expected_contains, description in tests:
            try:
                name, conf, color, derived = engine.classify_sample(sample, 'bone_collagen_qc')
                report.add_result(
                    f"Bone Collagen: {description}",
                    expected_contains in name,
                    details=f"C:N={sample['C_N_Ratio']} ‚Üí {name}"
                )
            except Exception as e:
                report.add_result(f"Bone Collagen: {description}", False, error=str(e))
    else:
        report.add_result("Bone Collagen QC scheme", False, details="Scheme not found")

    # Test 2: TAS Alkali Series
    if 'tas_alkali_series' in scheme_dict:
        tests = [
            ({"Na2O_wt+K2O_wt": 14.0, "SiO2_wt": 57.0}, "ALKALINE", "Phonolite"),
            ({"Na2O_wt+K2O_wt": 2.65, "SiO2_wt": 50.0}, "SUBALKALINE", "Tholeiite"),
            ({"Na2O_wt+K2O_wt": 7.0, "SiO2_wt": 55.0}, "SUBALKALINE", "Boundary"),
            ({"Na2O_wt+K2O_wt": 7.1, "SiO2_wt": 55.0}, "ALKALINE", "Just above boundary"),
        ]

        for sample, expected_contains, description in tests:
            try:
                name, conf, color, derived = engine.classify_sample(sample, 'tas_alkali_series')
                report.add_result(
                    f"TAS: {description}",
                    expected_contains in name,
                    details=f"Total alkali={sample.get('Na2O_wt+K2O_wt', '?')} ‚Üí {name}"
                )
            except Exception as e:
                report.add_result(f"TAS: {description}", False, error=str(e))
    else:
        report.add_result("TAS Alkali Series scheme", False, details="Scheme not found")

    # Test 3: Grain Size
    if 'sediment_grain_size' in scheme_dict:
        tests = [
            ({"GrainSize_um": 2.0}, "CLAY", "Clay"),
            ({"GrainSize_um": 4.0}, "SILT", "Silt lower boundary"),
            ({"GrainSize_um": 30.0}, "SILT", "Silt"),
            ({"GrainSize_um": 63.0}, "SAND", "Sand lower boundary"),
            ({"GrainSize_um": 150.0}, "SAND", "Sand"),
            ({"GrainSize_um": 2000.0}, "GRAVEL", "Gravel boundary"),
        ]

        for sample, expected_contains, description in tests:
            try:
                name, conf, color, derived = engine.classify_sample(sample, 'sediment_grain_size')
                report.add_result(
                    f"Grain Size: {description}",
                    expected_contains in name,
                    details=f"{sample['GrainSize_um']}¬µm ‚Üí {name}"
                )
            except Exception as e:
                report.add_result(f"Grain Size: {description}", False, error=str(e))
    else:
        report.add_result("Grain Size scheme", False, details="Scheme not found")

    # Test 4: Water Hardness
    if 'water_hardness' in scheme_dict:
        tests = [
            ({"Hardness_mgL": 20.0}, "SOFT", "Soft"),
            ({"Hardness_mgL": 60.0}, "MODERATELY", "Moderate boundary"),
            ({"Hardness_mgL": 90.0}, "MODERATELY", "Moderate"),
            ({"Hardness_mgL": 150.0}, "HARD", "Hard"),
            ({"Hardness_mgL": 250.0}, "VERY", "Very hard"),
        ]

        for sample, expected_contains, description in tests:
            try:
                name, conf, color, derived = engine.classify_sample(sample, 'water_hardness')
                report.add_result(
                    f"Water Hardness: {description}",
                    expected_contains in name.upper(),
                    details=f"{sample['Hardness_mgL']} mg/L ‚Üí {name}"
                )
            except Exception as e:
                report.add_result(f"Water Hardness: {description}", False, error=str(e))
    else:
        report.add_result("Water Hardness scheme", False, details="Scheme not found")

    # Test 5: USDA Soil Texture
    if 'usda_soil_texture' in scheme_dict:
        tests = [
            ({"Sand_pct": 10, "Silt_pct": 30, "Clay_pct": 60}, "CLAY", "Clay soil"),
            ({"Sand_pct": 92, "Silt_pct": 5, "Clay_pct": 3}, "SAND", "Sand soil"),
            ({"Sand_pct": 20, "Silt_pct": 60, "Clay_pct": 20}, "SILT LOAM", "Silt loam"),
        ]

        for sample, expected, description in tests:
            try:
                name, conf, color, derived = engine.classify_sample(sample, 'usda_soil_texture')
                report.add_result(
                    f"USDA Soil: {description}",
                    expected in name,
                    details=f"Sand={sample['Sand_pct']}%, Silt={sample['Silt_pct']}%, Clay={sample['Clay_pct']}% ‚Üí {name}"
                )
            except Exception as e:
                report.add_result(f"USDA Soil: {description}", False, error=str(e))
    else:
        report.add_result("USDA Soil Texture scheme", False, details="Scheme not found")


def test_batch_processing(report: TestReport):
    """Test batch processing capabilities"""

    if not TOOLKIT_AVAILABLE:
        report.add_result("Batch processing", False, error="Toolkit modules not available")
        return

    try:
        from data_hub import DataHub
        from engines.classification_engine import ClassificationEngine

        hub = DataHub()
        engine = ClassificationEngine()

        # Create test dataset
        samples = []
        for i in range(20):
            samples.append({
                'Sample_ID': f'BATCH{i:03d}',
                'Zr_ppm': 150 + (i * 5),
                'Nb_ppm': 10 + (i * 2),
                'C_N_Ratio': 3.2 if i < 10 else 4.5,
                'Notes': f'Test sample {i}'
            })

        count = hub.add_samples(samples)
        report.add_result(
            "Add batch samples",
            count == 20,
            details=f"Added {count} samples to DataHub"
        )

        # Test batch classification
        schemes = engine.get_available_schemes()
        if schemes:
            scheme_id = schemes[0]['id']

            try:
                results = engine.classify_all_samples(samples, scheme_id)
                report.add_result(
                    f"Batch classify with {schemes[0]['name']}",
                    len(results) == len(samples),
                    details=f"Classified {len(results)} samples"
                )

                # Count classifications
                classified = sum(1 for r in results if r['classification'] not in ['UNCLASSIFIED', 'INVALID_SAMPLE'])
                report.add_result(
                    "Batch classification rate",
                    True,
                    details=f"{classified}/{len(samples)} samples classified"
                )
            except Exception as e:
                report.add_result("Batch classification", False, error=str(e))

        # Test get_page
        try:
            page = hub.get_page(0, page_size=5)
            report.add_result(
                "Get page from DataHub",
                len(page) == 5,
                details=f"Page 0: {len(page)} samples"
            )
        except Exception as e:
            report.add_result("Get page", False, error=str(e))

        # Test get_selected
        try:
            selected = hub.get_selected([0, 2, 4])
            report.add_result(
                "Get selected samples",
                len(selected) == 3,
                details=f"Retrieved {len(selected)} selected samples"
            )
        except Exception as e:
            report.add_result("Get selected", False, error=str(e))

    except Exception as e:
        report.add_result("Batch processing setup", False, error=str(e))


# ============================================================================
# MAIN
# ============================================================================

def list_categories():
    """List available test categories"""
    print("\nüìã Available test categories:")
    print("  datahub     - Test DataHub functionality")
    print("  engine      - Test ClassificationEngine")
    print("  schemes     - Test JSON scheme files")
    print("  normalize   - Test column name normalization")
    print("  import      - Test file import")
    print("  ui          - Test UI component structure")
    print("  plugins     - Test plugin directory structure")
    print("  derived     - Test derived field calculations")
    print("  scientific  - Test scientific classifications")
    print("  batch       - Test batch processing")
    print("  all         - Run all tests (default)")
    print("\nUsage: python test_toolkit.py --category <category>")
    print("       python test_toolkit.py --verbose")
    print("       python test_toolkit.py --report (generate report from last run)")


def main():
    parser = argparse.ArgumentParser(description="Test Scientific Toolkit")
    parser.add_argument("--category", "-c", default="all",
                       help="Test category to run")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="Show verbose output")
    parser.add_argument("--list", "-l", action="store_true",
                       help="List available categories")
    parser.add_argument("--report", action="store_true",
                       help="Generate report from last run")

    args = parser.parse_args()

    if args.list:
        list_categories()
        return 0

    # Check if we're in the right directory
    if not Path("Scientific-Toolkit.py").exists() and not Path("data_hub.py").exists():
        print("\n‚ö†Ô∏è  Warning: Not seeing main toolkit files in current directory.")
        print("   Make sure you're running this from the Scientific Toolkit root folder.")
        print(f"   Current directory: {Path.cwd()}")
        response = input("\n   Continue anyway? (y/n): ")
        if response.lower() != 'y':
            return 1

    runner = TestRunner(verbose=args.verbose)
    runner.print_header("SCIENTIFIC TOOLKIT TEST SUITE")
    print(f"Toolkit available: {'‚úÖ' if TOOLKIT_AVAILABLE else '‚ùå'}")
    print(f"Current directory: {Path.cwd()}")

    # Run selected tests
    categories = {
        'datahub': test_data_hub,
        'engine': test_classification_engine,
        'schemes': test_scheme_files,
        'normalize': test_column_normalization,
        'import': test_file_import,
        'ui': test_ui_components,
        'plugins': test_plugin_structure,
        'derived': test_derived_fields_calculations,
        'scientific': test_scientific_classifications,
        'batch': test_batch_processing,
    }

    if args.category == 'all':
        for name, test_func in categories.items():
            runner.run_category(name, test_func)
    elif args.category in categories:
        runner.run_category(args.category, categories[args.category])
    else:
        print(f"\n‚ùå Unknown category: {args.category}")
        list_categories()
        return 1

    runner.print_summary()
    return 0


if __name__ == "__main__":
    sys.exit(main())
